"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ PROMETHEUS METRICS - Export de mÃ©triques MLOps
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“š OBJECTIF PÃ‰DAGOGIQUE
Ce module expose les mÃ©triques mÃ©tier de l'application au format Prometheus.
Il illustre comment instrumenter une application ML pour le monitoring production.

ğŸ”‘ CONCEPTS CLÃ‰S
- Types de mÃ©triques Prometheus : Counter, Gauge, Histogram
- Instrumentation automatique vs manuelle (FastAPI)
- Labels pour dimensions multiples (segmentation des donnÃ©es)
- Buckets pour histogrammes (distribution des valeurs)

ğŸ”— INTÃ‰GRATION
- AppelÃ© par : src/api/main.py (setup au dÃ©marrage)
- ConsommÃ© par : Prometheus (scrape /metrics toutes les 15s)
- Compatible V2 : s'ajoute au monitoring Plotly existant (complÃ©mentaire)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
from prometheus_client import Counter, Histogram, Gauge
from prometheus_fastapi_instrumentator import Instrumentator
import os
import sys  # ğŸ†• AJOUT

# Conditional import for Discord alerting
alert_high_latency = None
try:
    from src.monitoring.discord_notifier import alert_high_latency as _alert_high_latency
    alert_high_latency = _alert_high_latency
    print("âœ… Discord alert_high_latency imported", file=sys.stderr, flush=True)
except ImportError:
    print("âš ï¸ Discord alerting not available", file=sys.stderr, flush=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š MÃ‰TRIQUES CUSTOM - SpÃ©cifiques au modÃ¨le CV cats/dogs
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“ GAUGE : Valeur pouvant monter ET descendre (snapshot de l'Ã©tat actuel)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
database_status = Gauge(
    'cv_database_connected',
    'Database connection status (1=connected, 0=disconnected)'
)
# ğŸ’¡ USAGE
# - .set(1) : marque comme connectÃ©
# - .set(0) : marque comme dÃ©connectÃ©
#
# ğŸ¯ CAS D'USAGE
# - Monitoring santÃ© infrastructure (alerte si = 0)
# - CorrÃ©lation : Ã©checs prÃ©dictions â†” base dÃ©connectÃ©e ?
#
# ğŸ“ˆ QUERY PROMQL POUR ALERTE
# - cv_database_connected == 0 : dÃ©clenche alerte Discord

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Š COUNTER : Valeur toujours croissante (compte des Ã©vÃ©nements)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
predictions_total = Counter(
    'cv_predictions_total',
    'Total number of predictions',
    ['result', 'success']
)
# ğŸ’¡ LABELS
# - result : 'cat', 'dog', 'error'
# - success : 'true', 'false'
#
# ğŸ¯ USAGE
# predictions_total.labels(result='cat', success='true').inc()
#
# ğŸ“ˆ QUERY PROMQL
# - rate(cv_predictions_total[5m]) : prÃ©dictions par seconde
# - sum by (result)(cv_predictions_total) : total par classe

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Š HISTOGRAM : Distribution des valeurs (latence, confiance, etc.)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
inference_duration = Histogram(
    'cv_inference_duration_seconds',
    'Inference time in seconds',
    buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
)
# ğŸ’¡ BUCKETS
# DÃ©finissent les intervalles de temps (en secondes)
# [0.1, 0.25, 0.5, 1.0, ...] permet de mesurer :
# - Combien de prÃ©dictions < 100ms
# - Combien entre 100ms et 250ms
# - etc.
#
# ğŸ¯ USAGE
# with inference_duration.time():
#     result = model.predict(image)
#
# ğŸ“ˆ QUERY PROMQL
# - histogram_quantile(0.95, cv_inference_duration_seconds) : P95 latence
# - avg(cv_inference_duration_seconds_sum / cv_inference_duration_seconds_count) : moyenne

prediction_confidence = Histogram(
    'cv_prediction_confidence',
    'Model confidence score',
    labelnames=['result'],
    buckets=[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 1.0]
)
# ğŸ’¡ TRACKING CONFIANCE
# Permet de dÃ©tecter si le modÃ¨le devient moins sÃ»r â†’ drift potentiel
#
# ğŸ“ˆ QUERY PROMQL
# - histogram_quantile(0.5, cv_prediction_confidence) : mÃ©diane confiance

# ğŸ†• FEEDBACK UTILISATEUR
cv_user_feedback_total = Counter(
    'cv_user_feedback_total',
    'Nombre de feedbacks utilisateurs collectÃ©s',
    ['feedback_type']  # 'positive' ou 'negative'
)

def track_user_feedback(feedback_type: str):
    """
    Enregistre un feedback utilisateur
    
    Args:
        feedback_type: 'positive' (1) ou 'negative' (0)
    """
    try:
        valid_types = ['positive', 'negative']
        if feedback_type not in valid_types:
            print(f"âš ï¸  Invalid feedback_type: {feedback_type}. Expected: {valid_types}")
            return
        
        cv_user_feedback_total.labels(feedback_type=feedback_type).inc()
        print(f"âœ… Tracked user feedback: {feedback_type}")
    except Exception as e:
        print(f"âš ï¸  Failed to track feedback: {e}")

cv_last_inference_seconds = Gauge(
    'cv_last_inference_seconds',
    'Inference time (seconds) for the most recent request'
)

# ğŸ†• AVERAGE INFERENCE TIME
cv_avg_inference_seconds = Gauge(
    'cv_avg_inference_seconds',
    'Average inference time (seconds) for all predictions'
)

# ğŸ†• INFERENCE TIME IN MS FOR ALERTING
cv_inferencetime_ms = Gauge(
    'cv_inferencetime_ms',
    'Latest inference time in milliseconds (for alerting)'
)

def update_last_inference(duration: float):
    print(f"DEBUG: update_last_inference called with {duration}", file=sys.stderr, flush=True)
    try:
        # Update both metrics
        cv_last_inference_seconds.set(duration)
        cv_inferencetime_ms.set(duration * 1000)  # Convert seconds to milliseconds
        
        print(f"âœ… Updated inference metrics: {duration:.3f}s / {duration*1000:.0f}ms")
        
        # Check for high latency alert
        if alert_high_latency and duration * 1000 > 1000:  # 1000ms threshold
            print(f"ğŸš¨ High latency detected: {duration*1000:.0f}ms > 1000ms", file=sys.stderr, flush=True)
            alert_high_latency(duration * 1000, threshold=1000)
            
    except Exception as e:
        print(f"âš ï¸ Failed to update inference metrics: {e}", file=sys.stderr, flush=True)

# ğŸ†• COUNTER HTTP REQUESTS
from prometheus_client import Counter  # ğŸ†•

# counter des requÃªtes HTTP (label 'method' pour GET/POST)
cv_http_requests_total = Counter(
    "cv_http_requests_total",
    "Total number of HTTP requests processed by the CV app",
    ["method", "endpoint"]
)

def inc_http_request(method: str, endpoint: str) -> None:
    """
    IncrÃ©mente le compteur de requÃªtes HTTP.
    """
    try:
        cv_http_requests_total.labels(method=method.upper(), endpoint=endpoint).inc()
    except Exception:
        # ne pas planter l'app si Prometheus absent
        pass

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ SETUP - Configuration de l'instrumentation Prometheus
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def setup_prometheus(app):
    """
    Configure Prometheus pour FastAPI
    Compatible avec l'API existante V2
    
    ğŸ¯ INSTRUMENTATION AUTOMATIQUE
    Le Instrumentator ajoute automatiquement :
    - http_request_duration_seconds : latence par endpoint
    - http_requests_total : nombre de requÃªtes par status code
    - http_requests_in_progress : requÃªtes concurrentes
    
    ğŸ’¡ ENDPOINT /metrics
    ExposÃ© automatiquement au format Prometheus :
    # HELP cv_predictions_total Total number of predictions
    # TYPE cv_predictions_total counter
    cv_predictions_total{result="cat"} 42.0
    cv_predictions_total{result="dog"} 38.0
    
    Args:
        app: Instance FastAPI
    """
    if os.getenv('ENABLE_PROMETHEUS', 'false').lower() == 'true':
        # ğŸ“Š INSTRUMENTATION EN 2 Ã‰TAPES
        # 1. instrument(app) : ajoute middleware pour mÃ©triques auto
        # 2. expose(app, endpoint="/metrics") : crÃ©e route GET /metrics
        Instrumentator().instrument(app).expose(app, endpoint="/metrics")
        print("âœ… Prometheus metrics enabled at /metrics")
        
        # ğŸ’¡ FORMAT DE SORTIE /metrics
        # Texte brut (Content-Type: text/plain)
        # Scrapable par Prometheus toutes les 15s (cf. prometheus.yml)
    else:
        print("â„¹ï¸  Prometheus metrics disabled")
        # Utile en dev si on veut allÃ©ger le monitoring

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“ HELPERS - Fonctions de tracking appelÃ©es par l'API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def update_db_status(is_connected: bool):
    """
    Met Ã  jour le statut de la base de donnÃ©es
    
    ğŸ”— APPELÃ‰ PAR : healthcheck ou retry logic de connexion DB
    
    Args:
        is_connected: True si connexion PostgreSQL active
    
    ğŸ’¡ EXEMPLE D'INTÃ‰GRATION
    try:
        db.execute("SELECT 1")
        update_db_status(True)
    except Exception:
        update_db_status(False)
        # Alerte Grafana se dÃ©clenche automatiquement
    """
    database_status.set(1 if is_connected else 0)

def track_prediction(result: str, inference_time_ms: int, confidence: float, success: bool = True):
    """
    Track une prÃ©diction dans Prometheus
    
    ğŸ”— APPELÃ‰ PAR : /api/predict aprÃ¨s chaque infÃ©rence
    
    Args:
        result: 'cat', 'dog', ou 'error'
        inference_time_ms: Temps d'infÃ©rence en millisecondes
        confidence: Score de confiance (0.0 Ã  1.0)
        success: True si prÃ©diction rÃ©ussie
    
    ğŸ’¡ EXEMPLE D'INTÃ‰GRATION
    result = model.predict(image)
    track_prediction(
        result='cat',
        inference_time_ms=250,
        confidence=0.95,
        success=True
    )
    """
    # IncrÃ©menter compteur de prÃ©dictions
    predictions_total.labels(
        result=result,
        success=str(success).lower()
    ).inc()
    
    # Enregistrer temps d'infÃ©rence (conversion ms â†’ secondes)
    inference_duration.observe(inference_time_ms / 1000.0)
    
    # Mettre Ã  jour la moyenne d'infÃ©rence
    try:
        print(f"ğŸ” DEBUG: Calling track_inference_time with {inference_time_ms / 1000.0:.3f}s", file=sys.stderr, flush=True)
        track_inference_time(inference_time_ms / 1000.0)
        print(f"âœ… track_inference_time called successfully", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âŒ ERROR calling track_inference_time: {e}", file=sys.stderr, flush=True)
    
    # Enregistrer confiance du modÃ¨le
    if result != 'error':
        prediction_confidence.labels(result=result).observe(confidence)


# Variables globales pour le calcul de la moyenne
_inference_sum = 0.0
_inference_count = 0

def track_inference_time(duration: float):
    """Track inference time in histogram and update average."""
    global _inference_sum, _inference_count
    try:
        # Enregistre dans l'histogramme
        inference_duration.observe(duration)
        
        # Met Ã  jour les compteurs pour la moyenne
        _inference_sum += duration
        _inference_count += 1
        
        # Calcule et met Ã  jour la moyenne
        avg = _inference_sum / _inference_count
        cv_avg_inference_seconds.set(avg)
        
        print(f"âœ… Tracked inference {duration:.3f}s | Avg: {avg:.3f}s (n={_inference_count})", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âŒ ERROR tracking inference time: {e}", file=sys.stderr, flush=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“ CONCEPTS AVANCÃ‰S (pour aller plus loin)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# 1. MÃ‰TRIQUES SUPPLÃ‰MENTAIRES UTILES
#    - model_version (Gauge avec label 'version') : tracking dÃ©ploiements
#    - input_image_size (Histogram) : dÃ©tection images hors distribution
#    - gpu_memory_usage (Gauge) : monitoring ressources (si GPU disponible)
#
# 2. CARDINALITY (nombre de combinaisons de labels)
#    âš ï¸ Attention : trop de labels = explosion mÃ©moire Prometheus
#    Exemple Ã  Ã‰VITER : .labels(user_id=...) avec 1M users
#    Limite raisonnable : <10 valeurs par label
#
# 3. MÃ‰TRIQUES VS LOGS
#    - MÃ©triques : agrÃ©gÃ©es, numÃ©riques, queryable (dashboards, alertes)
#    - Logs : dÃ©taillÃ©s, textuels, debugging (ex: traceback erreurs)
#    Les deux sont complÃ©mentaires (pas l'un OU l'autre)
#
# 4. TESTS DES MÃ‰TRIQUES
#    import pytest
#    def test_track_prediction():
#        before = predictions_total._value.get()
#        track_prediction('cat', 100, 0.95)
#        assert predictions_total._value.get() == before + 1
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“š RESSOURCES PÃ‰DAGOGIQUES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# - Prometheus best practices: https://prometheus.io/docs/practices/naming/
# - Types de mÃ©triques expliquÃ©s: https://prometheus.io/docs/concepts/metric_types/
# - PromQL tutorial: https://prometheus.io/docs/prometheus/latest/querying/basics/
# - FastAPI Instrumentator: https://github.com/trallnag/prometheus-fastapi-instrumentator
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•